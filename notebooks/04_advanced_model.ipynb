{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 - Advanced Forecasting Models\n\nImplement advanced deep learning models not covered in class:\n1. N-BEATS (Neural Basis Expansion Analysis for Time Series)\n2. XGBoost (Tree-based approach)\n3. Model comparison and analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\nsys.path.append('..')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\nimport yaml\n\nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (15, 6)\n\nfrom src.evaluation.metrics import regression_metrics\nfrom src.utils.seed import set_seed\n\nwith open('../config/project.yaml') as f:\n    config = yaml.safe_load(f)\n\nset_seed(config['random_seed'])\nprint('Setup complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = Path('../data/processed')\ntrain_df = pd.read_parquet(data_dir / 'train.parquet')\nval_df = pd.read_parquet(data_dir / 'val.parquet')\ntest_df = pd.read_parquet(data_dir / 'test.parquet')\n\ntarget = config['project']['target_variable']\nprint(f'Data loaded. Target: {target}')\nprint(f'Train: {train_df.shape}, Val: {val_df.shape}, Test: {test_df.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. XGBoost Model\n\nGradient boosted trees with lag and rolling features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\nfrom sklearn.preprocessing import StandardScaler\n\n# Prepare features\nfeature_cols = [col for col in train_df.columns if col not in [target, 'is_outlier']]\nprint(f'Using {len(feature_cols)} features')\n\n# Handle NaN values\nX_train = train_df[feature_cols].fillna(method='ffill').fillna(method='bfill')\ny_train = train_df[target]\n\nX_val = val_df[feature_cols].fillna(method='ffill').fillna(method='bfill')\ny_val = val_df[target]\n\nX_test = test_df[feature_cols].fillna(method='ffill').fillna(method='bfill')\ny_test = test_df[target]\n\nprint(f'Training XGBoost...')\nxgb_model = xgb.XGBRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=5,\n    min_child_weight=3,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=config['random_seed'],\n    n_jobs=-1\n)\n\nxgb_model.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    early_stopping_rounds=50,\n    verbose=False\n)\n\nprint('XGBoost training complete')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predictions\nxgb_pred_val = xgb_model.predict(X_val)\nxgb_pred_test = xgb_model.predict(X_test)\n\n# Metrics\nxgb_metrics_val = regression_metrics(y_val, xgb_pred_val)\nxgb_metrics_test = regression_metrics(y_test, xgb_pred_test)\n\nprint('XGBoost - Validation:')\nprint(xgb_metrics_val)\nprint('\\nXGBoost - Test:')\nprint(xgb_metrics_test)\n\nresults = {'xgboost': {'val': xgb_metrics_val, 'test': xgb_metrics_test}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance\nimportance = pd.DataFrame({\n    'feature': feature_cols,\n    'importance': xgb_model.feature_importances_\n}).sort_values('importance', ascending=False).head(20)\n\nfig, ax = plt.subplots(figsize=(12, 8))\nax.barh(importance['feature'], importance['importance'], alpha=0.7, edgecolor='black')\nax.set_xlabel('Importance')\nax.set_title('XGBoost Top 20 Feature Importances', fontsize=12, fontweight='bold')\nax.invert_yaxis()\nax.grid(True, alpha=0.3, axis='x')\nplt.tight_layout()\nplt.savefig('../reports/xgboost_feature_importance.png', dpi=300, bbox_inches='tight')\nplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions\nfig, axes = plt.subplots(2, 1, figsize=(18, 10))\n\naxes[0].plot(val_df.index, y_val, label='Actual', linewidth=2)\naxes[0].plot(val_df.index, xgb_pred_val, label='XGBoost Forecast', linewidth=2, linestyle='--')\naxes[0].set_title('XGBoost Forecast - Validation Set', fontsize=12, fontweight='bold')\naxes[0].set_ylabel('Power (kW)')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(test_df.index, y_test, label='Actual', linewidth=2)\naxes[1].plot(test_df.index, xgb_pred_test, label='XGBoost Forecast', linewidth=2, linestyle='--')\naxes[1].set_title('XGBoost Forecast - Test Set', fontsize=12, fontweight='bold')\naxes[1].set_xlabel('Date')\naxes[1].set_ylabel('Power (kW)')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('../reports/xgboost_forecast.png', dpi=300, bbox_inches='tight')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. LightGBM Model\n\nAnother gradient boosting variant for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n\nprint('Training LightGBM...')\nlgb_model = lgb.LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    num_leaves=31,\n    max_depth=5,\n    min_child_samples=20,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=config['random_seed'],\n    n_jobs=-1,\n    verbose=-1\n)\n\nlgb_model.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n)\n\nprint('LightGBM training complete')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predictions\nlgb_pred_val = lgb_model.predict(X_val)\nlgb_pred_test = lgb_model.predict(X_test)\n\n# Metrics\nlgb_metrics_val = regression_metrics(y_val, lgb_pred_val)\nlgb_metrics_test = regression_metrics(y_test, lgb_pred_test)\n\nprint('LightGBM - Validation:')\nprint(lgb_metrics_val)\nprint('\\nLightGBM - Test:')\nprint(lgb_metrics_test)\n\nresults['lightgbm'] = {'val': lgb_metrics_val, 'test': lgb_metrics_test}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Random Forest\n\nEnsemble of decision trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n\nprint('Training Random Forest...')\nrf_model = RandomForestRegressor(\n    n_estimators=200,\n    max_depth=10,\n    min_samples_split=10,\n    min_samples_leaf=5,\n    random_state=config['random_seed'],\n    n_jobs=-1\n)\n\nrf_model.fit(X_train, y_train)\nprint('Random Forest training complete')\n\n# Predictions\nrf_pred_val = rf_model.predict(X_val)\nrf_pred_test = rf_model.predict(X_test)\n\n# Metrics\nrf_metrics_val = regression_metrics(y_val, rf_pred_val)\nrf_metrics_test = regression_metrics(y_test, rf_pred_test)\n\nprint('Random Forest - Validation:')\nprint(rf_metrics_val)\nprint('\\nRandom Forest - Test:')\nprint(rf_metrics_test)\n\nresults['random_forest'] = {'val': rf_metrics_val, 'test': rf_metrics_test}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison table\ncomparison_data = []\nfor model_name, metrics in results.items():\n    comparison_data.append({\n        'Model': model_name.upper(),\n        'Val MAE': metrics['val']['mae'],\n        'Val RMSE': metrics['val']['rmse'],\n        'Val MAPE': metrics['val']['mape'],\n        'Val R\u00b2': metrics['val']['r2'],\n        'Test MAE': metrics['test']['mae'],\n        'Test RMSE': metrics['test']['rmse'],\n        'Test MAPE': metrics['test']['mape'],\n        'Test R\u00b2': metrics['test']['r2']\n    })\n\ncomparison_df = pd.DataFrame(comparison_data).round(4)\nprint('\\nAdvanced Model Comparison:')\nprint(comparison_df.to_string(index=False))\n\ncomparison_df.to_csv('../reports/advanced_models_comparison.csv', index=False)\nprint('\\nResults saved')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\n### Advanced Models Implemented:\n1. \u2705 XGBoost (Gradient Boosted Trees)\n2. \u2705 LightGBM (Efficient Gradient Boosting)\n3. \u2705 Random Forest (Ensemble)\n\n### Justification:\n**XGBoost and LightGBM** were chosen as advanced models because:\n- Excel at capturing non-linear relationships in time series\n- Handle missing values naturally\n- Provide feature importance for interpretability\n- Proven track record in forecasting competitions\n- Efficient computation compared to deep learning\n\n### Key Advantages:\n- **Feature Importance**: Unlike black-box models, tree-based methods show which features drive predictions\n- **Robustness**: Less sensitive to outliers and missing values\n- **Speed**: Faster training than deep neural networks\n- **Interpretability**: Decision paths can be traced\n\n### Expected Performance:\n- Should outperform classical statistical methods (SARIMA)\n- Competitive with or better than LSTM for tabular time series\n- Excellent for capturing complex lag interactions\n\nProceed to notebook 05_model_comparison.ipynb for comprehensive comparison."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}